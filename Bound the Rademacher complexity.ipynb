{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                      Bound the Rademacher complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style=\"border-radius:10px\">We will proof that for any $H\\subseteq R^{n}$, with $R=sup_{h\\epsilon H}(\\sum_{i=1 }^{m}h^{2}_{i})^{1/2}$:\n",
    "\n",
    "$$\\hat{R_{m}}(A)=E_{\\sigma}[{sup}_{h\\epsilon H}(\\frac{1}{m}\\sum_{i=1 }^{m}h_{i}\\sigma_{i})]\\leqslant \\frac{R\\sqrt{2ln|A|}}{m}$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">For this proof we will used the Jensen's inequality for a convex function $\\phi(x)$:\n",
    "\n",
    ">$$\\phi(E(X))\\leqslant E(\\phi(X))$$\n",
    "\n",
    ">were $\\phi(x)=e^{x}$ and E(X) is the expected value of X. \n",
    "\n",
    ">We will also use Hoeffding's Inequality:\n",
    "\n",
    ">$$E[e^{sX}]\\leqslant e^{s^{2}(b-a)^2/8}$$\n",
    "\n",
    "\n",
    ">which can be used if $E[X]=0$ and $a\\leqslant X \\leqslant b$, and for any $s>0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{\\textit{proof}}$.\n",
    "\n",
    "$$exp(s\\hat{R}(A))=exp(sE_\\sigma[{sup}_{h\\epsilon H}\\sum_{i=1 }^{m}h_{i}\\sigma_{i}])\\leqslant E_\\sigma[exp(s*{sup}_{h\\epsilon H}\\sum_{i=1 }^{m}h_{i}\\sigma_{i})]=$$\n",
    "\n",
    "$$=E_{\\sigma}[{sup}_{h\\epsilon H}(exp(s\\sum_{i=1 }^{m}\\sigma_{i}h_{i}))]\\leqslant\\sum_{h\\epsilon H }E_{\\sigma}[exp(s\\sum_{i=1}^{m} \\sigma_{i}h_{i})]=\\sum_{h\\epsilon H}E_{\\sigma}[\\prod_{i=1}^{m}exp(s\\sigma_{i}h_{i})]=$$\n",
    "\n",
    "$$=\\sum_{h\\epsilon H}\\prod_{i=1}^{m}E_{\\sigma}[exp(s\\sigma_{i}h_{i})]$$\n",
    "\n",
    "(the first inequality came from Jensen's inequality)\n",
    "\n",
    "We can do this with $\\sigma_{i}$ since we know that they are independent.\n",
    "Now we can apply the Hoeffding's Inequality since $E_{\\sigma}[{\\sigma_{i}}h_{i}]=0$ where\n",
    "$\\beta-\\alpha=2h_{i}$ and $\\sigma_{i}h_{i}\\epsilon[\\alpha,\\beta]$.This gives us:\n",
    "\n",
    "$$exp(sE_{\\sigma}[{sup}_{h\\epsilon H}\\sum_{i=1}^{m}\\sigma_{i}h_{i}])\\leqslant \\sum_{h\\epsilon H}\\prod_{i=1}^{m}exp(\\frac{s^{2}(2h_{i})^{2}}{8})=$$\n",
    "\n",
    "$$=\\sum_{h\\epsilon H}exp(\\frac{s^{2}}{2}\\sum_{i=1}^{m}h_{i}^{2})\\leqslant|H|exp(\\frac{s^{2}R^{2}}{2})$$\n",
    "\n",
    "which is equal to:\n",
    "    \n",
    "$$E_{\\sigma}[{sup}_{h\\epsilon H}\\sum_{i=1 }^{m}\\sigma_{i}h_{i}] \\leqslant \\frac{ln|H|}{s}+\\frac{sR^{2}}{2}$$    \n",
    "\n",
    "Now we want to find the minimum of the right side.\n",
    "So, we define a function:\n",
    "    \n",
    "$$f(s)=\\frac{ln|H|}{s}+\\frac{sR^{2}}{2}$$ \n",
    "Taking the derivative and setting it equal to zero, we find:\n",
    "    \n",
    "$${f(s)}'=-\\frac{ln|H|}{s^{2}}+\\frac{R^{2}}{2}=0 \\Rightarrow $$\n",
    "\n",
    "$$ s=\\frac{\\sqrt{2ln|H|}}{R}$$\n",
    "\n",
    "Substitude this quantity back into the previous bound gives us:\n",
    "    \n",
    "$$E_{\\sigma}[{sup}_{h\\epsilon H}\\sum_{i=1 }^{m}\\sigma_{i}h_{i}] \\leqslant \\frac{Rln|H|}{\\sqrt{2ln|H|}}+ \\frac{R^{2}\\sqrt{2ln|H|}}{2R}=R\\sqrt{2ln|H|}$$\n",
    "\n",
    "Finally dividing both sides by m we get:\n",
    "\n",
    "$$E_{\\sigma}[{sup}_{h\\epsilon H}(\\frac{1}{m}\\sum_{i=1 }^{m}h_{i}\\sigma_{i})]\\leqslant \\frac{R\\sqrt{2ln|A|}}{m}$$                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style = \"border-radius:10px\">**Some thing i would like to add:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The proof of the Hoeffding's Inequality:\n",
    "\n",
    ">$$E[e^{sX}]\\leqslant e^{s^{2}(b-a)^2/8}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\textit{proof}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$e^{sx}$ is convex, we can write:\n",
    "\n",
    "$$ e^{sx} \\leqslant\\frac{x-a}{b-a} e^{sb}+z\\frac{b-x}{b-a}e^{sa}$$\n",
    "\n",
    "Letting $p=-\\frac{a}{b-a}$ and $\\phi(u)=-pu+ln(1-p+pe^{u})$,we have:\n",
    "\n",
    "$$E[e^{sX}] \\leqslant\\frac{b}{b-a}e^{sa}-\\frac{a}{b-a}e^{sb}=(1-p)e^{-ps(b-a)}-pe^{s(b-a)}e^{-ps(b-a)}=$$\n",
    "\n",
    "$$=((1-p)+pe^{s(b-a)})e^{-ps(b-a)}=$$\n",
    "\n",
    "$$=e^{\\varphi(s(b-a))}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the proof we only need an upper bound for $\\varphi(u)$, and we will\n",
    "do this using Taylor expansion:\n",
    "\n",
    "$${\\varphi(u)}'=-p+\\frac{pe^{u}}{1-p+pe^{u}}=-p+\\frac{p}{p+(1-p)e^{-u}}$$\n",
    "\n",
    "and \n",
    "\n",
    "$${\\varphi(u)}''=\\frac{p(1-p)e^{-u}}{(p+(1-p)e^{-u})^{2}} \\leqslant1/4$$\n",
    "\n",
    "So, we get that:\n",
    "\n",
    "$${\\varphi(u)}=\\varphi(0)+u{\\varphi(0)}'+\\frac{u^{2}}{2}{\\varphi(t)}''\\leqslant 0+0+\\frac{u^{2}}{2}(1/4)\\leqslant\\frac{u^{2}}{8}$$\n",
    "\n",
    "And this gives $\\varphi(s(b-a)) \\leqslant \\frac{s^{2}(b-a)^{2}}{8}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Proof of Jensen's inequality:\n",
    ">$$E(\\phi(X))\\geqslant \\phi(E(X))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\phi$ is convex we have:\n",
    "\n",
    "$$\\phi(x)\\geqslant\\phi(x_{0})+b(x-x_{0})$$\n",
    "\n",
    "where b is the slope of the tangent.Setting $x=X$ and $x_{0}=E[X]$ the inequality becomes:\n",
    "\n",
    "$$\\phi(X)\\geqslant\\phi(E[X])+b(X-E[X])$$\n",
    "\n",
    "Taking the expected value of both sides of the inequality and using the fact that the expected value operator preserves inequalities, we obtain\n",
    "\n",
    "$$E[\\phi(X)]\\geqslant E[\\phi(E[X])+b(X-E[X])]=$$ (by linearity of the expected value)\n",
    "\n",
    "$$=\\phi(E[X])+b(E[X]-E[X])=\\phi(E[X])$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
